
\subsection{Linearity of Expectation}
%TODO
%move RV add to this section

\ssn{Motivating problems}
\begin{itemize}
\item I toss a coin 20 times.  What is the expected number of times that I will find pairs of consecutive H? (In this counting, by the way, I count every pair: a sequence THHHT will count as two occurrences of pairs of consecutive heads.) 
\item I draw a smooth curve on some squared paper. How might I estimate the length of the curve from the number of grid lines it crosses? 
\item I toss a D6 10 times.  What is the probability that the sum of the rolls is 30? 
\end{itemize}
\end{n}


\ssn{Fundamental situations}
Reviewing what we have done, there are two fundamental situations. 
\begin{itemize}
\item For a \emph{discrete random variable} $X$, we have a finite or countable set $S=\{ x_1, x_2, x_3, \dots \}$ of numerical outcomes with corresponding probabilities $(p_1, p_2, p_3, \dots )$.
 
\item For a \emph{continuous random variable} $X$ we have a \emph{probability density function (pdf)} $f_X: \RR  \map [0,\infty)$  such that 
 \[
  \PP(u\leq X \leq v) = \int_{u}^{v} f_X(x) \dd x
 \]
In many cases $X$ takes values only in some interval $[a,b]$ and the pdfs are  zero outside the interval. 

We also have the \emph{cumulative distribution function (cdf)}
 \[
    F_X(x) = \PP(X \leq x) = \int_{-\infty}^x f_X(u)\, \dd u
 \]
so that $f_X(x) = F_X'(x)$.  
\end{itemize}
\end{n}

\ssn{Expected value}
As we have seen before, the expected value of $X$ is defined in the discrete case as 
 \[
  \EE(X) \; {=}\;  \sum_j \PP(X=x_j)x_j \; = \; p_1 x_1 +  p_2 x_2 + \dots . 
  \]
The expected value is defined in the continuous case as  
\[
 \EE(X) \stackrel{\text{def}}{=}  \int_{-\infty}^{\infty} x \, f_X(x) \,\dd x.
 \]
In both cases, there are distributions for which the expected value does not exist (i.e.\ it is  ``infinite''). 
\end{n}

\ssn{Expected values of functions of $X$}
Let $g$ be a function of $X$. Then $g(X)$ is itself a random variable and we can take its expected value: 
 \[
  \EE(g(X)) {=}  \PP(X=x_1) g(x_1) +  \PP(X=x_2) g(x_2) + \dots . 
  \]
 in the discrete case and 
\[
 \EE(g(X)) {=}  \int_{-\infty}^{\infty} g(x) f_X(x) \dd x.
 \]
In both cases, it may be that $\EE(g(X))$ does not exist even if $\EE(X)$ does. 
\end{n}

\ssn{Examples}
\begin{itemize}
\item The expected value of a constant is itself:  $\EE(b) = b$. The expected value of $X+b$ where $b$ is a constant is just $\EE(X) + b$. (Here, $X+b$ means the random variable obtained by taking a sample from $X$ and adding the number $b$ to it.)  
\item Let $X$ be the random variable that is the roll of a D5.  Then $\EE(X)=3$.  Suppose we want to compute $\EE(X^2)$.  Then we evaluate
 \[
    \frac15 1^2 + \frac15 2^2 +\frac15 3^2 +\frac15 4^2 +\frac15 5^2= \frac{55}5 = 11.
 \]
 Note by the way that generally $\EE(X^2) \not= (\EE(X))^2$. 
 \item Consider the continuous random variable $X$ on $[0,1]$ with $f_X(x) = 2x$.  Then 
  \[
    \EE(X) = \int_0^1 x (2x) \,\dd x = \frac23, \qquad 
      \EE(e^X) = \int_0^1 e^x (2x) \,\dd x = 2
  \]
 (where the final integral is evaluated ``by parts''). 
\end{itemize}
\end{n}

\ssn{Theorem}
\label{exlin}
Let $X$ and $Y$ be random variables with finite expected values. 
\tcb 
Then 
  \[
      \EE( X+Y) = \EE(X)  + \EE(Y).
  \]
 If also $X$ and $Y$ are independent, then 
  \[
       \EE(XY) = \EE(X) \EE(Y)  
  \]
 and more generally (assuming the expectations exist), 
   \[
       \EE(f(X) g(Y)) = \EE(f(X)) \EE(g(Y)). 
  \]
 \etcb 
\begin{proof} (A bit more challenging than some of our proofs: you may want to come back to it later.) 
We give a proof only for the discrete case now.  Let $X$ taking values $x_i$ with probabilities $p_i$ and $Y$ taking values $y_j$ with probabilities $q_j$ be two random variables.   Then 
 \[
     \sum_{j} \PP(\text{$X=x_i$ and $Y=y_j$}) = \PP(X=x_i)
 \]
because the event $X=x_i$ is the disjoint union of all the events ``$X=x_i$ and $Y=y_j$'' over $j$.   

Now consider 
\begin{eqnarray*}
 \EE(X+Y) &=& \sum_{\text{all $i,j$}}  \PP(\text{$X=x_i$ and $Y=j_j$}) (x_i+y_j)  \\
 &=& \sum_{i} \sum_j   \PP(\text{$X=x_i$ and $Y=x_j$}) x_i +
        \sum_{j} \sum_i   \PP(\text{$X=x_i$ and $Y=j_j$}) y_j \\
  &=&  \sum_{i}    \PP(\text{$X=x_i$}) x_i +
        \sum_{j}   \PP(\text{$Y=j_j$}) y_j \\
  &=& \EE(X) + \EE(Y).
\end{eqnarray*}

Turning to the second claim, recall that $X$ and $Y$ being independent means that 
 \[
   \PP(\text{$X=x_i$ and $Y=y_j$}) = \PP(X=x_i) \, \PP(Y=y_j). 
 \]
Then 
\begin{eqnarray*}
  \EE(XY) &=& \sum_i \sum_j \left(  \PP(\text{$X=x_i$ and $Y=y_j$}) x_iy_j \right) \\
  &=& \sum_i \sum_j \left( \PP(X=x_i) \, \PP(Y=y_j)  x_iy_j \right) \\
  &=& \left( \sum_i \PP(X=x_i)x_i \right) \left( \sum_j \PP(Y=y_j)  y_j \right) \\
  &=& \EE(X) \, \EE(Y). 
\end{eqnarray*}
The third claim follows by an analogous calculation. 
\end{proof}
\end{n}


