\section{Covariance and correlation} \label{s10} 

\ssn{Learning Outcomes}
After studying this week you will be able to:
\begin{itemize}
\item compute covariances and correlations; 
\item explain what correlation tells us about the relationship between random variables; 
\item use covariance in problems involving adding variances. 
\end{itemize}
\end{n}

\subsection{Preparation for the week}

\ssn{Motivating problem}
\begin{itemize}
\item How can we measure the degree to which two random variables fail to be independent? 
\end{itemize}
\end{n}


\ssn{Introduction}
Two discrete random variables $X,Y$ are independent if and only if for all $k,l$ we have 
 \[
    \PP( \text{$X=k$ and $Y=l$} ) = \PP(X=k) \, \PP(Y=l). 
 \]
Also, if two random variables are independent, then 
 \[
           \EE(XY) = \EE(X) \EE(Y). 
 \]
\end{n}

\ssn{Definition}
Let $X,Y$ be random variables.   Then the \emph{covariance} of $X$ and $Y$ is defined (assuming the expected values exist)  by 
\tcb 
 \[
       \cov (X,Y) = \EE(XY) - \EE(X) \EE(Y). 
 \]
 \etcb 
 
 \noindent 
 Therefore if $X$ and $Y$ are independent then $\cov(X,Y)=0$.
 (BUT be aware that the converse is not true.) 
\end{n}

\ssn{Alternative version}
Suppose that $\EE(X) = \mu_X, \, \EE(Y) = \mu_Y$. Then the  covariance can also be expressed as  
\[
     \cov (X,Y) = \EE \left(( X - \mu_X)( Y- \mu_Y)\right). 
  \]
\begin{proof}
 \begin{eqnarray*}
 \EE (( X - \mu_X)( Y- \mu_Y)) &=& 
 \EE( XY - \mu_X Y - \mu_Y X + \mu_X \mu_Y  ) \\
 &=& \EE(XY) - \mu_X \, \EE(Y) - \mu_Y \,\EE(X) + \mu_X \mu_Y\\
 &=& \EE(XY) - \EE(X)\, \EE(Y) 
 \end{eqnarray*}
  \end{proof}
This version helps explain the meaning of covariance. The quantity $( X - \mu_X)( Y- \mu_Y)$ is positive if $X$ and $Y$ are both greater than their expected value or both less. On the other hand it is negative if one of the two is above its expected value and one is below.  So the covariance is positive if $X$ and $Y$ tend to increase and decrease together, and it is negative if one increasing tends to be associated with the other decreasing. 
\end{n}



\ssn{Properties} \hfill 

  \tcb 
\begin{itemize}
\item $\cov(X,Y) = \cov(Y,X)$
\item $\cov(X,X) = \var(X)$ 
\item $\cov( aX+bY, Z) = a \, \cov(X,Z) + b\, \cov(Y,Z)$ 
\end{itemize}
\etcb 

\noindent
The first two properties are straightforward to check.  The third, which states that covariance is linear in its first variable, also follows directly from the definition and properties of expectation. 

Combining the first and third properties, we can prove that covariance is linear also in its second variable. 
\end{n}

\ssn{Correlation}
 Let $X,Y$ be random variables. Then (assuming the variances and covariance exist) we define the \emph{correlation} of $X,Y$ to be 
 \tcb 
 \[
          \cor(X,Y) = \frac{ \cov(X,Y)}{\sqrt{\var(X) \var(Y)}}. 
  \]
  \etcb 

\noindent 
We will see later that $-1 \leq \cor(X,Y) \leq 1$. 
\end{n}

\sse{Magic coins}
Consider the magic coins from the start of Week 9. Changing notation, we will let $X$ and $Y$ be random variables which are equal to 1 if their coin is H and 0 if it is T. See Table~\ref{mc2}.
\begin{table}[h] 
\centering  \[  \arraycolsep=6pt\def\arraystretch{2.0} 
\begin{array}{|c|cc|}
 \hline 
   & X=1 & X=0 \\ 
  \hline
Y=1 &  \frac4{12} & 
      \frac1{12}    \\
Y=0 &  \frac2{12} & \frac5{12}  \\
\hline
\end{array} 
\] 
\caption{Magic coins.  \label{mc2}}
\end{table}
Compute the covariance and correlation of $X$ and $Y$. 
(Answers:  $\EE(X) = \EE(X^2) = 1/2$, $\var(X)=1/4$, $\EE(Y) = \EE(Y^2) = 5/12$, $\var(Y) = 35/144$, $\EE(XY) = 1/3$, $\cov(X,Y) = 1/8$, $\cor(X,Y) = 3/\sqrt{35} \approx 0.507$. The moderately large, positive correlation is because three quarters of the time, the two coins come down the same.) 
 \end{e}

\sse 
If you did not work through the covariance calculation examples in workshop 5, it is suggested you do so now. 
\end{e}

\sse
Use the properties of covariance to show that if $Y = mX+c$ (where $m\not=0$ and $c$ are constants) then 
 \[
    \cov(Y,X) = m \var(X), \qquad \cor(Y,X)= \pm1 \text{ (negative if $m<0$)}
 \]
\end{e}

\pagebreak 

\subsection{Notes} 

\ssn{Theorem }
Suppose $X,Y$ are random variables. Then 
\tcb 
\[
  \var(X+Y) = \var(X) + \var(Y) + 2 \, \cov(X,Y).
  \]
 \etcb 
 \begin{proof}
  \begin{eqnarray*} 
  \var(X+Y) &=& \EE( (X+Y)^2) - (\EE(X+Y) )^2 \\
    &=& \EE( X^2 + 2XY + Y^2 )  - (\EE(X) + \EE(Y))^2 \\
   &=& \EE(X^2) + \EE(Y^2) + 2\EE(XY) - \EE(X^2) -\EE(Y^2) - 2 \EE(X)\EE(Y) \\
   &=& \var(X) + \var(Y) + 2 \, \cov(X,Y).
 \end{eqnarray*} 
 \end{proof}
 \noindent 
 This result generalises our previous result stating that variances add for \emph{independent} random variables. 
\end{n}



\ssn{A vectors analogy}
It is possible to think of covariance as being a sort of ``scalar product'' for probability distributions.   In that context, $\var(X)$ is the ``squared length'' of the distribution.  There is an analogy too of the Cauchy-Schwarz theorem ($(\mathbf u \cdot \mathbf v)^2 \leq ||\mathbf u||^2 \, ||\mathbf v ||^2$).  
\end{n}

\ssn{Theorem}
For random variables $X,Y$ of finite variance, 
\tcb \[
          \left( \cov(X,Y) \right)^2 \leq \var(X) \, \var(Y).
   \]
  \etcb 
 \begin{proof}
  For a parameter $t$ consider the random variable $tX - Y$. Then 
  \[
     \var(tX-Y) = \var(X) t^2 - 2t \, \cov(X,Y) + \var(Y).
  \]
 For all $t$ we have $\var(tX+Y) \geq 0$ and so thinking of the right-hand side above as a quadratic in $t$, it cannot have two real roots.  But the ``$b^2-4ac \leq 0$'' condition then gives us 
  \[
      \left(  \cov(X,Y)\right)^2 - \var(X) \, \var(Y) \leq 0. 
  \]
 \end{proof}
\noindent 
This has an important corollary: it implies that 
\tcb 
\[
        -1 \leq \cor(X,Y)  \leq 1
 \]
\etcb
and indeed that $| \cor(X,Y) | = 1$ implies that for some $t$ we have $\var(tX-Y) = 0$ and hence $tX-Y$ is constant. In other words, $Y=mX+c$ for some constants $m,c$. 
\end{n}


\ssn{Example/Exercise}
Consider tossing a coins three times. Let $X$ denote the number of heads; let $Y$ denote the number of pairs of consecutive heads (with the usual rules that e.g.\ HHH counts as two such pairs) ad let $Z$ denote the number of consecutive pairs of tails.   Compute $\cov(X,Y)$ and $\cov(X,Z)$.  (Note we would expect the first to be positive and the second to be negative (the more heads, the more pairs of heads but the fewer pars of tails)). Calculate also the associated correlations.   (My calculations: $\EE(X)=3/2, \EE(Y)=\EE(Z) = 1/2$.  Also $\EE(X^2) = 3; \EE(Y^2)= \EE(Z^2) = 3/4$.  Thus $\var(X)=3/4, \var(Y)=\var(Z) = 1/2$. Finally, $\EE(XY)=5/4, \EE(XZ)=1/4$.     So $\cov(X,Y)= 1/2, \cov(X,Z)=-1/2$.  Thus $\cor(X,Y) = -\cor(X,Z) = \sqrt{2/3}$.) 
\end{n}

\sse{} 
Suppose $\var(X) = \var(Y)$ and $\var(X+Y) = k\var(X)$.  Evaluate $\cor(X,Y)$ in each of the cases $k=0,1,2,3,4$. 
\end{e} 

\sss 
Set  $\cor(X,Y)=r$.  Then since $\var(X) = \var(Y)$ we have 
 \[
 \var(X+Y) = 2 \var(X) + 2 \cov(X,Y), \qquad 
    \cor(X,Y) = \frac{\cov(X,Y)}{\var(X)} = r. 
 \]
 So 
 \[
      \var(X+Y) = 2 (1+r) \var(X) .
 \]
 Thus the cases $k=0,1,2,3,4$ correspond to $\cor(X,Y)$ being $-1,-1/2,0,1/2,2$ respectively. 
\end{s} 