\section{Conditioning, random walks and inclusion-exclusion} \label{s7} 

\ssn{Learning Outcomes}
After studying this week you will be able to:
\begin{itemize}
\item Solve second order difference equations with constant coefficients;
\item use the technique of conditioning on the first step to solve random walk and similar problems;
\item use inclusion-exclusion for more than two sets. 
\end{itemize}
\end{n}

\subsection{Preparation for the week}



\ssn{Sequences and recursion relations} 
We will need to work with sequences defined by what are known as ``difference equations'' or ``recursion relations''. For example, consider the sequence $x_k$ where 
 \[
      x_0 = 1, \quad x_{k+1} = 2 x_k  \quad\text{(for $k \geq 0$.)}
 \]
The sequence $x_0, x_1, x_2, \dots$ generated is $1,2,4,8,16,\dots$ and by inspecton a direct formula for the sequence is $x_k = 2^k$. 

We could also specify a sequence by a \emph{second order constant-coefficient difference equation} such as  
 \[
      x_{k+1} - 4 x_k + 3 a_{k-1} = 0, \quad x_0 = 0, \; x_1 = 1. 
 \]
Working out a couple of terms we discover $x_2 = 4, x_3 = 13$ and $x_4=30$. Because the equation determines a term from the two preceding ones, it requires us to have two ``initial values'' ($x_0$ and $x_1$) to determine the sequence. 

A way of solving such equations is to look for solutions of the form $x_k = \lambda^k$, where $\lambda$ is a constant to be determined.  Substituting in to the equation and cancelling $\lambda^{k-1}$ we discover $\lambda$ must satisfy the quadratic equation 
 \[
     \lambda^2 - 4 \lambda + 3 = (\lambda -1)(\lambda -3) = 0. 
 \]
(Notice the coefficients are the same as in the original equation.) 
If you substitute $x_k = 3^k$ back into the recurrence relation, you discover that it solves the equation (but not the initial conditions). Similarly $ x_k = 1^k = 1$ satisfies the equation. 

Because these equations are \emph{linear}, any linear combination of solutions is a solution. Thus for any constants $a,b$ we have that 
 \[
      x_k = a \, 1^k + b \, 3^k \; = \; a + b \, 3^k 
 \]
is a solution.  In fact, one can show that it is the general solution. 
We can thus solve our recurrence relation problem by using the initial conditions to fix $a$ and $b$.  

Substituting $k=0$ and $k=1$ in this case, we require 
 \[
       a+b = 0, \quad a + 3b = 1
 \]
which has the solution $a = -1/2$ and $b= 1/2$. Thus the sequence generated by our recursion relation and initial conditions is
 \[
     x_k  = \frac12 3^k - \frac12. 
 \]

This method even works if the quadratic that arises has complex roots (which is not something we are likely to need). The solutions that arise are always of the form $x_k = a \lambda_1^k + b \lambda_2^k$ where $\lambda_1, \lambda_2$ are te roots of the associated quadratic equation. 

There is however one special case.  It may be that the quadratic does not have two roots $\lambda_1, \, \lambda_2$ but has a single real, repeated root $\lambda$.  In that case, the general solution turns out to be 
 \[
        x_k = ( a k + b) \lambda^k. 
 \] 
\end{n} 

\sse{} 
  Solve 
  \[
     x_{k+1} - 5 x_k + 6 x_{k-1} = 0, \quad x_0 = 1, \; x_1 = 0.
  \]
 (You can check your answer by generating the first couple of terms by hand.) 
\end{e} 

\sss{} 
The roots of the quadratic are $2, 3$ and the solution is $x_k = 3 \, 2^k - 2 \, 3^k$.
\end{s} 

\sse{} 
  Solve 
  \[
     x_{k+1} - 4 x_k + 4 x_{k-1} = 0, \quad x_0 = 1, \; x_1 = 3.
  \]
 (This is the ``repeated root'' case. You can check your answer by generating the first couple of terms by hand.) 
\end{e} 

\sss{} 
The repeated root of the quadratic is  $\lambda = 2$ and the solution is $x_k =  ( (1/2) k + 1) 2^k$.
\end{s} 

\sse{} For amusement, solve 
  \[
     x_{k+1} - 2 x_k + 2 x_{k-1} = 0, \quad x_0 = 0, \; x_1 = 1.
  \]
 This is the ``complex roots'' case. The first few terms are 
  \[
    0, 1, 2, 2, 0, -4, -8, -8, 0, 16, 32, 32, 0, -64, -128, \dots
  \]
 Find the formula! 
\end{e} 

\sss{} 
The roots are $1 \pm i$ and the solution with these initial conditions is \[
     x_k = \frac{-i}{2} (1+i)^k + \frac{i}{2} (1-i)^k. 
\]
\end{s} 

\ssn{Conditioning on the first step}
This is a useful trick that enables us to compute probabilities and expectations almost by magic.  It relies on the Law of Total Probabilities both in the vanilla form and the expectations version.  

Consider the following problem.  Let $X$ be the number of times I have to toss a fair coin until I first get two consecutive H. We will compute $\EE(X)$ and to ease things we will give it a name $\EE(X) = s$. 

Consider now three ways our sequence of tosses might start. 
\[
   T \dots   \qquad HT\dots \qquad HH
\]
where there are no dots after the final option because we have our two consecutive H in that case $X=2$ and we stop.  The probabilities of these three different beginnings are $1/2, 1/4, 1/4$ respectively and they cover all possibilities.  

The law of total probabilities for expectations now says
 \[
   \EE(X) = \PP(T\dots) \EE(X \st T\dots) + 
           \PP(HT\dots) \EE(X \st HT\dots) + 
            \PP(HH,\dots) \EE(X \st HH) .
 \]
The expectation on the left-hand side is $s$ (which is what we are trying to calculate).  For $\EE(X \st T\dots)$ notice that having tossed a T, we are in the same position as when we started, \emph{except we have already used up a toss}.  So $\EE(X \st T\dots) = s+1$.  By an identical argument, $\EE(X \st HT\dots) = s+2$.  And finally $\EE(X \st HH) = 2$ because we have already achieved our two consecutive H. 

Substituting these in we get an equation for $s$ that we can solve. (Exercise for the reader which you are advised to do before the online quiz.) 
\end{n}

 
\subsection{Notes} 


\ssn{Example}
I roll a D6. What is the probability $p$ that I get the first `6' before I roll any odd numbers? 

 Let $W$ be the event that I roll a 6' before any odd numbers. 
Let $A$ be the event that my first roll is odd. Let $B$ be the event that my first roll is a `6'.   Let $C$ be the event that my first roll is a `2' or a `4'.  

Then $\PP(W \st A) = 0$ and $\PP(W \st B) = 1$.  On the other hand, rolling a `2'or a `4' has no effect on my likelihood of achieving $W$.   So $\PP(W \st C) = p$. 
So
\begin{eqnarray*}
 p = \PP(W) &=& \PP(W \st A) \PP(A) + \PP(W \st B) \PP(B) + 
 \PP(W \st C) \PP(C) \\
  &=&  0 \,\frac12 + 1 \, \frac16 + p \frac13.
\end{eqnarray*}
Solving, $p=1/4$. 

(In fact, there is a smarter way of solving this problem.  We can rephrase it as asking whether of the numbers 1,3,5,6, the 6 appears first. Clearly any one of the four are equally likely to appear first.) 
\end{n}

\ssn{Random walks on $[0,n]$}
Consider a random walk on the set $[0,1,2,\ldots, n]$, meaning that one starts somewhere and then at each step independently moves one step to the right (i.e.\  from $k$ to $k+1$) with probability $p$ and to the left (i.e.\  from $k$ to $k-1$) with probability $1-p$.   The positions $0$ and $n$ are \ul{absorbing} meaning that when one reaches one of them you stop and the walk is finished.

Our first question is what is the probability starting from $k$ that one ends up being absorbed at $0$ rather than at $n$.  Let $x_k$ be that probability. Then conditioning on the first move we obtain the equation 
 \[
    x_{k} = p x_{k+1} + (1-p) x_{k-1} 
 \] 
or equivalently
  \[
     p x_{k+1} - x_k + (1-p) x_{k-1} = 0, \quad x_0=1, \; x_n=0
 \]  
where we have added boundary conditions saying that if we are at 0 then the probability of being absorbed at 0 is 1 and that if we are at n then the probability of being absorbed at 0 is 0. 

This 2nd order difference equation has associated polynomial 
 \[
      p \lambda^2 - \lambda + (1-p) = (p\lambda - (1-p))(\lambda -1) 
 \]
which provided $p \not=1/2$ has two roots: $\lambda = 1$ and $\lambda = q/p$ where $q=1-p$.   We discussed the case $p=1/2$ in lectures. 

So the general solution for $x_k$ is 
 \[
     x_k = a + b \left(\frac{q}{p}\right)^k 
 \]
Solving the two simultaneous equations (exercise) we get 
 \[
  \displaystyle     x_k = \frac{\left(\frac{q}{p}\right)^k - \left(\frac{q}{p}\right)^n }{1 - \left(\frac{q}{p}\right)^n}. 
 \]
 In the special case $p=1/2$ the result (from lectures) is 
  \[
             x_k = 1 - \frac{k}{n} 
  \]
\end{n}

\ssn{Wandering for ever} 
A question we have been avoiding is whether the random walk might in fact just wander forever in the interior and never reach either boundary.  In fact, clearly it can do that if e.g.\ it starts near the middle and alternates left and right moves.  So we have to ask how likely it is.  

If we reverse the boundary conditions so that we are computing the probability of being absorbed at $n$, we discover in both cases that the probability of absorption at $n$ is $1-x_k$ ans so the probability of wandering forever in the interior is zero. 
\end{n}

\sse
We could ask about the case where there is no right-hand limit (as we did in lectures for the case $p=1/2$ where we found $x_k=1$ for all $k$). 

Show that 
\[
     \lim_{n \map \infty} x_k = \begin{cases} 
         1 & \text{if} \;  p  < 1/2 \\ 
         \left( \frac{q}{p} \right)^k & \text{if} \;  p  > 1/2. 
     \end{cases}
\]
This suggests that this might be the solution of the ``no right endpoint'' case but does not prove it because it is not clear that the solution with no end point is equal to this limit.   But these answers are indeed correct. 
\end{e}


\ssn{Indicator functions}  Given a subset $A \subseteq S$ of a sample space, we define its \emph{indicator function} $I_A$ which is the function on $S$ defined by 
 \[
    I_A(x) = \begin{cases}   1 & x \in A \\ 0 & x \not\in A.
     \end{cases} 
 \]
\end{n}

\ssn{Use of indicator functions} 
We can think of $I_A$ as being a random variable: it takes the value $1$ if the event happens and $0$ if it does not.   So,
 \[
 \EE(I_A) = \PP(A).
 \]
With this trick we can turn many probability problems into random variable problems.  
\end{n}

\ssn{Properties} Let $A,B$ be subsets of a sample space $S$. 
\begin{enumerate}
\item $I_A^2 = I_A$ (and more generally $I_A^k = I_A$ when $k \in \mathbb{N}$). 
\item $I_{A^c} = 1 - I_A$
\item $I_{A\cap B} = I_A I_B$ (and in particular if $A \subseteq B$ then 
$I_A I_B = I_A$).
\end{enumerate}
\end{n}

\ssn{The general inclusion-exclusion principle} \label{giep}
Writing the general version is fairly intimidating: 
\[
 \PP(A_1 \cup A_2 \cup\dots\cup A_n) =
  \sum_{k=1}^n  (-1)^{k+1}  
 \sum_{1\leq i_1 < i_2 <\dots < i_k \leq n}
\PP( A_{i_1} \cap A_{i_2} \cap \dots \cap A_{i_k}  )
\]
\begin{proof} 
We give a proof for the $n=3$ case using indicator functions. 

Now suppose $M = A \cup B \cup \dots \cup C$. Then the function 
 \[
   (I_M - I_A) (I_M - I_B) (I_M - I_C) = 0 
  \]
 because for $x \not\in M$ all the terms vanish and for $x \in M$ we know $x$ is in at least one of $A,B,C$ and the corresponding term vanishes. 
 Multiplying out and rearranging:
  \[
    I_M^3 = I_M^2 (I_A+I_B+I_C) - I_M (I_A I_B + I_A I_C + I_B I_C 0 
       + I_A I_B I_C. 
  \]
Now, $I_MI_A = I_A$ since $A$ is a subset of $M$ and similarly for $B$ and $C$. So the ``$I_M$''s on the right-hand side can be dropped. And on the left we can replace $I_M^3$ by $I_M$.  Then 
 \[
  I_M = (I_A+I_B+I_C) - ( I_{A\cap B} + I_{A \cap C} + I_{B \cap C}) 
    + I_{A \cap B \cap C}.
\]
Taking expectations of both sides (they are random variables) and using the linearity from \ref{exlin} we arrive at
 \[
  \PP(M) = (\PP(A) + \PP(B) + \PP(C) ) - 
   (\PP(A \cap B) + \PP(A\cap C) + \PP(B \cap C) ) 
    + \PP( A \cap B \cap C). 
 \]
\end{proof}

\end{n}


\subsection{Exercises and problems}

\sse 
Evaluate
\begin{enumerate}
\item Let $X$ be the result of rolling a D6. What is the probability of $X=6$ given that $X$ is even?  What is the expected value of $X$ given that $X$ is even? 
\item Rolling a D6, let $Y$ be the number of rolls it takes until I roll a `6'. What is the probability that $Y=5$ given that $Y>3$? 
\item Rolling a D6 again, what is the expected total number of `6's I roll in 8 attempts, given that my first two rolls are 6,2?  
\end{enumerate}
Answers: (1) $1/3$, $4$; (2) $5/36$; (3) $2$.  
\end{e}

\sse{} 
Consider a random walk (equally likely to move in each direction) on the vertices of a regular $n$-gon. Choose two adjacent vertices.  Starting at one of the chosen vertices, what is the expected number of moves until you reach the other?   Try $n=3,4,5$ for a start.    
\end{e}

\ssp{} This (optional) problem solves the question about ``stopping at the largest number'' from the first lecture in the limit as $n\map \infty$.  We will play fast and loose with limits, and so this derivation is not entirely rigorous. The problem is equivalent to the following. 

Consider numbers $x_0,x_1,x_2,\dots,x_{n-1}$ where the numbers are in \emph{decreasing} order.  Now arrange the $n$ numbers in a list in a random order.  Choose a number $0 < \alpha < 1 $.  Consider the ``initial segment'' consisting of the first $\alpha N$ numbers in the list (of course, $\alpha n$ is not necessarily an integer, but choose the integer that's closest). 

Let the RV $K$ be defined by $K=k$ if $x_k$ is the largest number in the initial segment.    Now find the first number in the list that is larger than $x_K$: what is the probability $\PP(W)$ where $W$ is the event that this is $x_0$, the largest of all the numbers?  

We will answer this in the limit as $n \map \infty$. We calculate $p$ by conditioning on the value of $k$ where $k$ is the smallest value such that $x_k$ is in the initial segment. 
\begin{enumerate}
    \item Explain why $\PP(W \st k=0) = 0$.
    \item Explain why in the limit $n \map \infty$ we have $\PP(K=k) = \alpha (1-\alpha)^k$ for $k < n-k$. 
    \item Explain why $\PP(W \st K=k) = \frac1k$ for $k \geq 1$.
    \item Justify (as far as you can) the claim that  as $n \map \infty$ we have 
     \[
        \PP(W) \approx \alpha \sum_{k=1}^\infty (1-\alpha)^k \frac1k.  
     \]
     \item Comparing with the Taylor series for $\log(1-x)$, deduce that \[
           \PP(W) \approx - \alpha \log(\alpha).
     \]
     \item Use calculus to deduce that $\PP(W)$ is maximised by taking $\alpha = 1/e$ and the maximum value is also $1/e$. 
\end{enumerate}
\end{e}
